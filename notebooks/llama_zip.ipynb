{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzTMsFMDc-L-"
      },
      "outputs": [],
      "source": [
        "#@markdown # Installation\n",
        "#@markdown Run this cell to install `llama-zip` with the necessary dependencies. You will have two options for the installation:\n",
        "#@markdown 1. **Regular Installation (CPU Only)**: This option will install `llama-cpp-python` with support for CPU inference only. This will only take a minute or two, but compression and decompression speeds will be limited (1-2 tokens/sec).\n",
        "#@markdown 2. **Enable GPU Support (Takes ~15 Minutes)**: This option will install `llama-cpp-python` with CUDA support for faster inference using the GPU. This will take around 15 minutes, and compression and decompression speeds will be much faster (20+ tokens/sec).\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "button_cpu = widgets.Button(description=\"Regular Installation (CPU Only)\", button_style='primary', layout=widgets.Layout(width='300px'))\n",
        "button_gpu = widgets.Button(description=\"Enable GPU Support (Takes ~15 Minutes)\", button_style='warning', layout=widgets.Layout(width='300px'))\n",
        "\n",
        "def on_button_cpu_clicked(b):\n",
        "    clear_output()\n",
        "\n",
        "    !git clone https://github.com/alexbuz/llama-zip.git\n",
        "    %cd llama-zip\n",
        "    %pip install .\n",
        "\n",
        "    clear_output()\n",
        "    inf('CPU Only Installation Done!', 'success', '300px')\n",
        "\n",
        "def on_button_gpu_clicked(b):\n",
        "    clear_output()\n",
        "\n",
        "    !git clone https://github.com/alexbuz/llama-zip.git\n",
        "    %cd llama-zip\n",
        "    %pip install .\n",
        "\n",
        "    !CMAKE_ARGS=\"-DLLAMA_CUDA=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --no-cache-dir --verbose\n",
        "\n",
        "    clear_output()\n",
        "    inf('GPU Support Enabled!', 'success', '300px')\n",
        "\n",
        "button_cpu.on_click(on_button_cpu_clicked)\n",
        "button_gpu.on_click(on_button_gpu_clicked)\n",
        "\n",
        "display(button_cpu, button_gpu)\n",
        "\n",
        "def inf(msg, style, wdth):\n",
        "    inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth))\n",
        "    display(inf)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # LLM Download\n",
        "#@markdown Run this cell to download an LLM. This is necessary for `llama-zip` to function, as the LLM guides the compression and decompression process.\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "llm_options = {\n",
        "    \"Meta-Llama-3-8B (Q8_0)\": \"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF/resolve/main/Meta-Llama-3-8B.Q8_0.gguf\",\n",
        "    \"Meta-Llama-3-8B (Q4_K_M)\": \"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF/resolve/main/Meta-Llama-3-8B.Q4_K_M.gguf\",\n",
        "}\n",
        "\n",
        "dropdown_llm = widgets.Dropdown(\n",
        "    options=[\"Select an LLM...\"] + list(llm_options.keys()),\n",
        "    description=\"LLM:\"\n",
        ")\n",
        "\n",
        "def on_llm_select(change):\n",
        "    global llm_path\n",
        "    if change[\"new\"] != \"Select an LLM...\":\n",
        "        clear_output()\n",
        "\n",
        "        selected_llm = change[\"new\"]\n",
        "        llm_url = llm_options[selected_llm]\n",
        "        llm_path = llm_url.split(\"/\")[-1]\n",
        "\n",
        "        if not os.path.isfile(llm_path):\n",
        "            !wget {llm_url}\n",
        "\n",
        "        clear_output()\n",
        "        inf(f\"{selected_llm} Downloaded!\", \"success\", \"300px\")\n",
        "\n",
        "dropdown_llm.observe(on_llm_select, names=\"value\")\n",
        "\n",
        "display(dropdown_llm)\n",
        "\n",
        "def inf(msg, style, wdth):\n",
        "    inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth))\n",
        "    display(inf)\n"
      ],
      "metadata": {
        "id": "_Kqg6ibCS5xS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Start Interactive Mode (Recommended)\n",
        "!llama-zip $llm_path -i"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DNOnwChzd7IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Compress Text\n",
        "Text = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!llama-zip $llm_path -c \"$Text\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "QqypLcS2dYZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Decompress Text\n",
        "Text = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!llama-zip $llm_path -d $Text"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4q4UcjaMeAxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“¦ Repo: [GitHub](https://github.com/AlexBuz/llama-zip)\n",
        "### Original Colab notebook by laVashik\n",
        "### Improved by AlexBuz"
      ],
      "metadata": {
        "id": "h1dXmnAxkeGo"
      }
    }
  ]
}